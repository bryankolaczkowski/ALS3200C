{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "convolution_shapes.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPECnMtzD5oRnowurhe6Etd"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LVPZpmQuoXQ"
      },
      "source": [
        "# building convolution neural networks\n",
        "\n",
        "Convolution neural networks are a bit 'trickier' than the simple densely-connected networks we've been looking at so far. This is because convolutions:\n",
        "\n",
        "* work with data having a more complex shape, such as 2D image data with multiple channels\n",
        "* typically reduce the size of the input\n",
        "* generate multiple outputs\n",
        "\n",
        "In this jupyter notebook, we'll build some convolution neural networks, in order to get a practical understanding of how to specify various features of the network, and how design decisions impact the network's shape and parameter count."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ey7bPJHdDPG1"
      },
      "source": [
        "First, let's assume we want to build a convolution neural network that can process color images of size 64x64 pixels.\n",
        "\n",
        "For a color image, we typically assume the data are encoded in RGB format, with 3 image channels. So, the shape of our input image data will be:\n",
        "\n",
        "    (64,64,3)\n",
        "\n",
        "for a 64x64 pixel image in RGB format.\n",
        "\n",
        "We therefore need to specify the input shape for the first layer in our network as:\n",
        "\n",
        "    input_shape=[64,64,3]\n",
        "\n",
        "to match the shape of the input image data (remember the batch dimension does not need to be specified for tensorflow).\n",
        "\n",
        "For image data, we will use a \"2-dimensional\" convolution, even though there are multiple 'channels' in the image data. Two-dimensional convolutions are implemented in tensorflow as a tf.keras.layers.Conv2D object.\n",
        "\n",
        "To create a Conv2D object, we need to specify the number of \"filters\" we want to use, and the \"kernel_size\" of the filters.\n",
        "\n",
        "The number of filters is a user-selected option, and it can really be anything you like. A typical convolution layer might have anywhere from 32 to 512 filters, or more. For now, we'll just specify a single filter, so we'll need to set the option:\n",
        "\n",
        "    filters=1\n",
        "\n",
        "when we create the Conv2D object.\n",
        "\n",
        "The kernel size is also up to the user. However, nearly all Conv2D kernels are square, having equal height and width, and the height and width of the kernel is almost always an odd number. Most convolution networks use kernel size of (3,3), (5,5) or (7,7). Early in the development of convolution networks, there was more of a diversity of kernel sizes, and a trend toward using *larger* kernels. More recently, more emphasis has been placed on building *deep* networks consisting of hundreds or thousands of convolution layers, and kernel sizes tend to be *smaller* to reduce parameter counts. In contemporary convolution networks, (3,3) convolutions are the most commonly used 'default'.\n",
        "\n",
        "To specify the kernel size, we just set the option:\n",
        "\n",
        "    kernel_size=(3,3)\n",
        "\n",
        "when we create the Conv1D object.\n",
        "\n",
        "The following code cell creates a simple 1-filter convolution neural network layer with a (3,3) kernel size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHFhIfALulj1"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Conv2D(filters=1, kernel_size=(3,3), input_shape=[64,64,3]))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoDGu530unmX"
      },
      "source": [
        "We can see that the output shape of the convolution layer (ignoring the batch dimension) is (62,62,1). The last \"1\" is the number of fiters, which we specified when creating the layer.\n",
        "\n",
        "The height and the width of the layer's output are both 62, which are calculated from the shape of the input image (64x64) and the size of the kernel (3,3).\n",
        "\n",
        "Specifically, we can calculate the output shape of a 2D convolution layer as follows:\n",
        "\n",
        "    outwidth = (inwidth - kernelwidth) + 1  = (64 - 3) + 1  = 61 + 1 = 62\n",
        "\n",
        "There are 28 trainable paramters in our 2D convolution layer. Given a kernel size of (3,3), this means that each filter will take inputs from a 3x3 'grid' of image pixels. So, there are 3x3=9 inputs to each filter. Each input requires an input weight, so there are 9 trainable input weights for each filter in our convolution layer. However there are *3* input channels in our image data, and each input channel requires an *independent* convolution filter. So, given the 3 input channels, our model should have 9x3=27 trainable parameters. But let's not forget the bias term! In tensorflow, the bias term is shared among all channels, so it only adds a single trainable paramter: 27+1=28.\n",
        "\n",
        "If we increased the number of filters to 2, we'd have 28x2=56 trainable parameters, and the output shape of the convolution layer would be (62,62,2).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9yvhlI0MJ-C"
      },
      "source": [
        "XX"
      ]
    }
  ]
}