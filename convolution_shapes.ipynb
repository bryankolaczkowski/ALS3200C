{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "convolution_shapes.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNbzrEuwxc8O76meQ5fnLnz"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LVPZpmQuoXQ"
      },
      "source": [
        "# building convolution neural networks\n",
        "\n",
        "Convolution neural networks are a bit 'trickier' than the simple densely-connected networks we've been looking at so far. This is because convolutions:\n",
        "\n",
        "* work with data having a more complex shape, such as 2D image data with multiple channels\n",
        "* typically reduce the size of the input\n",
        "* generate multiple outputs\n",
        "\n",
        "In this jupyter notebook, we'll build some convolution neural networks, in order to get a practical understanding of how to specify various features of the network, and how design decisions impact the network's shape and parameter count."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ey7bPJHdDPG1"
      },
      "source": [
        "First, let's assume we want to build a convolution neural network that can process color images of size 64x64 pixels.\n",
        "\n",
        "For a color image, we typically assume the data are encoded in RGB format, with 3 image channels. So, the shape of our input image data will be:\n",
        "\n",
        "    (64,64,3)\n",
        "\n",
        "for a 64x64 pixel image in RGB format.\n",
        "\n",
        "We therefore need to specify the input shape for the first layer in our network as:\n",
        "\n",
        "    input_shape=[64,64,3]\n",
        "\n",
        "to match the shape of the input image data (remember the batch dimension does not need to be specified for tensorflow).\n",
        "\n",
        "For image data, we will use a \"2-dimensional\" convolution, even though there are multiple 'channels' in the image data. Two-dimensional convolutions are implemented in tensorflow as a tf.keras.layers.Conv2D object.\n",
        "\n",
        "To create a Conv2D object, we need to specify the number of \"filters\" we want to use, and the \"kernel_size\" of the filters.\n",
        "\n",
        "The number of filters is a user-selected option, and it can really be anything you like. A typical convolution layer might have anywhere from 32 to 512 filters, or more. For now, we'll just specify a single filter, so we'll need to set the option:\n",
        "\n",
        "    filters=1\n",
        "\n",
        "when we create the Conv2D object.\n",
        "\n",
        "The kernel size is also up to the user. However, nearly all Conv2D kernels are square, having equal height and width, and the height and width of the kernel is almost always an odd number. Most convolution networks use kernel size of (3,3), (5,5) or (7,7). Early in the development of convolution networks, there was more of a diversity of kernel sizes, and a trend toward using *larger* kernels. More recently, more emphasis has been placed on building *deep* networks consisting of hundreds or thousands of convolution layers, and kernel sizes tend to be *smaller* to reduce parameter counts. In contemporary convolution networks, (3,3) convolutions are the most commonly used 'default'.\n",
        "\n",
        "To specify the kernel size, we just set the option:\n",
        "\n",
        "    kernel_size=(3,3)\n",
        "\n",
        "when we create the Conv1D object.\n",
        "\n",
        "The following code cell creates a simple 1-filter convolution neural network layer with a (3,3) kernel size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHFhIfALulj1"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Conv2D(filters=1, kernel_size=(3,3), input_shape=[64,64,3]))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoDGu530unmX"
      },
      "source": [
        "We can see that the output shape of the convolution layer (ignoring the batch dimension) is (62,62,1). The last \"1\" is the number of fiters, which we specified when creating the layer.\n",
        "\n",
        "The height and the width of the layer's output are both 62, which are calculated from the shape of the input image (64x64) and the size of the kernel (3,3).\n",
        "\n",
        "Specifically, we can calculate the output shape of a 2D convolution layer as follows:\n",
        "\n",
        "    outwidth = (inwidth - kernelwidth) + 1  = (64 - 3) + 1  = 61 + 1 = 62\n",
        "\n",
        "There are 28 trainable paramters in our 2D convolution layer. Given a kernel size of (3,3), this means that each filter will take inputs from a 3x3 'grid' of image pixels. So, there are 3x3=9 inputs to each filter. Each input requires an input weight, so there are 9 trainable input weights for each filter in our convolution layer. However there are *3* input channels in our image data, and each input channel requires an *independent* convolution filter. So, given the 3 input channels, our model should have 9x3=27 trainable parameters. But let's not forget the bias term! In tensorflow, the bias term is shared among all channels, so it only adds a single trainable paramter: 27+1=28.\n",
        "\n",
        "If we increased the number of filters to 2, we'd have 28x2=56 trainable parameters, and the output shape of the convolution layer would be (62,62,2).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9yvhlI0MJ-C"
      },
      "source": [
        "Larger kernel sizes combine more information from a larger 'block' of the input image, but they also require more trainable parameters, and they 'shrink' the layer's output to a greater degree.\n",
        "\n",
        "For example, the following code cell uses a (7,7) kernel to analyze the same 64x64 image data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CI12KKO_FAbE"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Conv2D(filters=1, kernel_size=(7,7), input_shape=[64,64,3]))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWn62W5ZFGL8"
      },
      "source": [
        "There are now 148 trainable parameters in the model. A 7x7 convolution kernel has 7x7=49 trainable input weights. There are 3 input channels, so incorporating information from all 3 channels takes 49x3=147 parameters. Plus the single shared bias term, so 148 total.\n",
        "\n",
        "Also notice that the *output* shape of the convolution layer has gotten smaller:\n",
        "\n",
        "    (64 - 7) + 1 = 58\n",
        "\n",
        "If we start combining *many* convolution layers in a feed-forward network, the resulting output can shrink pretty quickly:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziK2IDK5F2e3"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Conv2D(filters=1, kernel_size=(7,7), input_shape=[64,64,3]))\n",
        "model.add(tf.keras.layers.Conv2D(filters=1, kernel_size=(7,7)))\n",
        "model.add(tf.keras.layers.Conv2D(filters=1, kernel_size=(7,7)))\n",
        "model.add(tf.keras.layers.Conv2D(filters=1, kernel_size=(7,7)))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxPcO7AzGEN_"
      },
      "source": [
        "To combat this 'shrinking data' problem, we can ask tensorflow to 'draw' an appropriate 'border' around our input data, so that the shape of the convolution layer's output remains the *same* as the shape of the input (except the channel dimension, which is always specified by the number of convolution filters in the layer).\n",
        "\n",
        "This 'border' is called \"padding\". Padding essentially 'extends' the height and width of the 'image' with zeros on all sides. The number of extra rows and columns of zeros is determined automatically by the kernel size.\n",
        "\n",
        "To initiate padding, all we need to do is set the:\n",
        "\n",
        "    padding='same'\n",
        "\n",
        "option when we create a convolution layer.\n",
        "\n",
        "This is demonstrated in the following code cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkPn_ehNGwdm"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Conv2D(filters=1, kernel_size=(7,7), padding='same', input_shape=[64,64,3]))\n",
        "model.add(tf.keras.layers.Conv2D(filters=1, kernel_size=(7,7), padding='same'))\n",
        "model.add(tf.keras.layers.Conv2D(filters=1, kernel_size=(7,7), padding='same'))\n",
        "model.add(tf.keras.layers.Conv2D(filters=1, kernel_size=(7,7), padding='same'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kevaYBoKGvdJ"
      },
      "source": [
        "With \"same\" padding on *all* the Conv2D layers, the 'height' and 'width' of the image data remains constant at 64x64, no matter how many convolution layers we process the image data through. This can be very useful, particularly if we want to build a very 'deep' convolution network consisting of many layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a2gVIcYHmiQ"
      },
      "source": [
        "## convolution image classification\n",
        "\n",
        "Now that we have some basic understanding of how to build convolution layers in tensorflow, let's put this into 'practice' and build an image classifier.\n",
        "\n",
        "For this problem, we are going to classify images from a 'standard' neural-network training and evaluation data set called \"CIFAR10\". The CIFAR10 data set comes from the Canadian Institute for Advanced Research; it has been widely used to develop, train and evaluate image-classification AI systems.\n",
        "\n",
        "The CIFAR10 data set consists of 60,000 32x32 color images in RGB format. Each image comes from 1 of 10 possible classes: airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks. There are 6000 images from each of the 10 possible classes.\n",
        "\n",
        "The CIFAR10 data set is so widely used that tensorflow has a function that will *automatically* download the data set for us, including splitting the data set into appropriate training and validation sub-sets.\n",
        "\n",
        "The following code cell will download the training and validation data from the CIFAR10 data set, normalize the images so the pixel values are between 0 and 1, and display a few example images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kohd7ugSKbaV"
      },
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# download CIFAR10 data\n",
        "(train_images, train_labels), (valid_images, valid_labels) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "# Normalize pixel values to be between 0 and 1\n",
        "train_images = train_images / 255.0 \n",
        "valid_images = valid_images / 255.0\n",
        "\n",
        "# plot example images\n",
        "class_names = ['airplane', 'automobile', 'bird',  'cat',  'deer',\n",
        "               'dog',      'frog',       'horse', 'ship', 'truck']\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "for i in range(25):\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(train_images[i])\n",
        "    plt.xlabel(class_names[train_labels[i][0]])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdwA6xHrK9Bx"
      },
      "source": [
        "It probably takes a few seconds to download the entire 60,000 images.\n",
        "\n",
        "As you can see from the examples, there is *not* much 'high-def' information in a 32x32 color image! You can probably make out the 'true' class assigned to each image, but it can be a bit tough, as the images are quite blurry and 'pixelated' at such low resolution.\n",
        "\n",
        "But let's see how a convolution network does."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phVdz5BZMVbV"
      },
      "source": [
        "XX"
      ]
    }
  ]
}