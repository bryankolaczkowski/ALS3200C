{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "stop_early.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPUqU0++BPIU5zifiW9PVi9"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6nNsB7piGpF"
      },
      "source": [
        "# stopping training early to mitigate overfitting\n",
        "\n",
        "During stochastic gradient descent and other similar training processes, the model will typically tend to 'fit' the training data better and better as training proceeds.\n",
        "\n",
        "At the beginning of the training process, the model's parameters are initialized with 'random' values, so the model usually doesn't 'fit' the training data at all.\n",
        "\n",
        "The gradient information provided by backpropagation allows the model's parameter values to quickly improve the model's fit to the training data over the first few epochs of updates. Depending on the number of parameters in the model, this can happen very quickly (for small models) or it can take a longer amount of time (for larger, more complex models).\n",
        "\n",
        "In many cases, during the first part of the training process, the model 'learns' to adjust its parameter values so they accurately represent the *patterns* in the training data. Only *after* the patterns in the data have been fit does the model begin to 'overfit' the error in the data.\n",
        "\n",
        "If we could somehow 'diagnose' when the model stops fitting the *patterns* in the data and *starts* to overfit the *error* in the data, we could potentially *stop* the training process early, *before* overfitting occurs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dX92QrVt0RRl"
      },
      "source": [
        "Run the code cell below, which simulates 100 data samples, splits them into 60% training and 40% validation data, and fits a fairly-complex neural network model to the data for 200 training epochs.\n",
        "\n",
        "Obseve the model's \"loss\" on the training data and its \"val_loss\" on the validation data over the course of the training run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnTTz2D8iCAW"
      },
      "source": [
        "import sklearn.datasets\n",
        "import sklearn.model_selection\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "# simulate data\n",
        "x,y = sklearn.datasets.make_regression(n_samples=100,\n",
        "                                       n_features=2,\n",
        "                                       bias=100.0,\n",
        "                                       noise=10.0,\n",
        "                                       random_state=201188)\n",
        "y /= 100.0\n",
        "\n",
        "# partition into train and validation subsets\n",
        "train_x, valid_x, train_y, valid_y = sklearn.model_selection.train_test_split(x,\n",
        "                                                                              y,\n",
        "                                                                              test_size=40,\n",
        "                                                                              random_state=221882)\n",
        "\n",
        "# package training and validation data into tensorflow Dataset objects\n",
        "train_data = tf.data.Dataset.from_tensor_slices((train_x, train_y)).batch(10)\n",
        "valid_data = tf.data.Dataset.from_tensor_slices((valid_x, valid_y)).batch(10)\n",
        "\n",
        "# build model\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Dense(units=4, activation=tf.keras.activations.relu, input_shape=[2]))\n",
        "model.add(tf.keras.layers.Dense(units=4, activation=tf.keras.activations.relu))\n",
        "model.add(tf.keras.layers.Dense(units=1))\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.SGD(),\n",
        "              loss=tf.keras.losses.MeanSquaredError())\n",
        "model.summary()\n",
        "\n",
        "# fit model\n",
        "model.fit(train_data, epochs=200, validation_data=valid_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcp2dFQiiFO5"
      },
      "source": [
        "You may need to run the training process a few times, to get a feeling for when and how overfitting will occur in this example. Given that the data set is very small and the batch size is also small, each training run is likely to be highly variable.\n",
        "\n",
        "However, by the end of each run, you'll likely notice that the model's \"loss\" on the training data is quite a bit lower than its \"val_loss\" on the validation data, suggesting the model is overfitting."
      ]
    }
  ]
}